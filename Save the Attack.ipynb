{"cells":[{"cell_type":"markdown","source":["# **SAVE THE ATTACK**\n","\n","*   **Designed classification model to correctly identify vulnerable machine**\n","*   **Performed Exploratory Data Analysis and preprocessing on big data having 7.1 million data rows followed by classificaIon models like Decision Tree and Random Forest**\n","*   **Implemented Bagging and Boosting techniques that helped to achieve 62% accuracy**\n","\n","*Designed and Developed By: Ashutosh Soni*"],"metadata":{"id":"x_VM-oZ0JvKI"},"id":"x_VM-oZ0JvKI"},{"cell_type":"markdown","source":["# Import Necessary Libraries"],"metadata":{"id":"RlqgOYxkKjZK"},"id":"RlqgOYxkKjZK"},{"cell_type":"code","execution_count":null,"id":"6426f4f0","metadata":{"id":"6426f4f0","outputId":"23fdaa6f-a9aa-4e7b-c749-8eef05207dc7"},"outputs":[{"data":{"text/plain":["'2.6.0'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.model_selection as model_selection\n","import sklearn.linear_model as linear_model\n","import sklearn.svm as svm\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import BaggingClassifier\n","import sklearn.metrics as metric\n","import sklearn.preprocessing as preprocessing\n","from scipy.linalg import svd\n","import pickle\n","import missingno as msno\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","from sklearn.decomposition import IncrementalPCA\n","\n","import tensorflow as tf\n","tf.__version__"]},{"cell_type":"markdown","source":["# Data Preprocessing\n","\n","**Definition of pre-processing class that helps to perform some necessary tasks to clean dataset. Some of the techniques implemented are as follows:**\n","\n","\n","1.   Removing unneccesary columns\n","2.   Missing value management\n","3.   Removal of duplicate entried\n","4.   Transformation of Data Ditribution\n","5.   Outlier Removal\n","6.   PCA\n","\n"],"metadata":{"id":"yEyF8uGyKsnY"},"id":"yEyF8uGyKsnY"},{"cell_type":"code","execution_count":null,"id":"8a7fffed","metadata":{"id":"8a7fffed"},"outputs":[],"source":["class preproc:\n","    ## Outlier Removal class\n","    class OutlierRemoval: \n","        def __init__(self, lower_quartile, upper_quartile):\n","            self.lower_whisker = lower_quartile - 1.5*(upper_quartile - lower_quartile)\n","            self.upper_whisker = upper_quartile + 1.5*(upper_quartile - lower_quartile)\n","        \n","        def removeOutlier(self, x):\n","            return (x if x <= self.upper_whisker and x >= self.lower_whisker else (self.lower_whisker if x < self.lower_whisker else (self.upper_whisker)))\n","    \n","    def __init__(self):\n","        self.cols_to_drop=[]\n","        self.l_encoder={}\n","        ## Observed categorical attributes of the data\n","        self.categorical_cols=['EngineVersion', 'AppVersion', 'AvSigVersion', 'OsPlatformSubRelease', 'OsBuildLab', \n","                               'SkuEdition', 'Census_MDC2FormFactor', 'Census_PrimaryDiskTypeName', \n","                               'Census_ChassisTypeName', 'Census_PowerPlatformRoleName', 'Census_OSVersion', \n","                               'Census_OSBranch', 'Census_OSEdition', 'Census_OSSkuName', 'Census_OSInstallTypeName', \n","                               'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName', \n","                               'Census_ActivationChannel', 'Wdft_IsGamer']\n","        self.std_scaler=preprocessing.StandardScaler()\n","        self.pca=IncrementalPCA(68, whiten=True)\n","        self.robScaler=preprocessing.RobustScaler()\n","        self.qTransform=preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)\n","        \n","        for col in self.categorical_cols:\n","            self.l_encoder[col]=LabelEncoder()        \n","    \n","    ## preprocessing helper function for train data\n","    def train_preprocess(self, df):\n","        self.cols_to_drop=['MachineIdentifier', 'ProductName', 'IsBeta', 'HasTpm', 'DefaultBrowsersIdentifier', 'OrganizationIdentifier', \n","                           'PuaMode', 'SmartScreen', 'Census_ProcessorClass', 'Census_InternalBatteryType', \n","                           'Census_IsFlightingInternal', 'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled', 'SMode',\n","                           'Platform', 'OsVer', 'Processor', 'IsProtected', 'AutoSampleOptIn', 'Firewall', 'UacLuaenable',\n","                           'Census_DeviceFamily', 'Census_PrimaryDiskTotalCapacity', 'Census_SystemVolumeTotalCapacity',\n","                           'Census_TotalPhysicalRAM', 'Census_OSArchitecture', 'Census_IsPortableOperatingSystem', \n","                           'Census_IsFlightsDisabled', 'Census_FlightRing', 'Census_IsVirtualDevice', 'Census_IsPenCapable',\n","                           'Census_IsAlwaysOnAlwaysConnectedCapable'\n","                          ]\n","        df.drop(axis='columns', labels=self.cols_to_drop, inplace=True)\n","        print(\"shape after dropping unnecesary columns: \", df.shape)\n","        \n","        ## Handling missing values and categorical valued attributes\n","        print(\"Attributes count that have missing values before missing value handle: \")\n","        count=0\n","        for col in df.columns:\n","            if(df[col].isna().sum()>0):\n","                count+=1\n","        print(\"Total attributes that have missing values: \", count)\n","\n","        for col in df.columns:\n","            if(col!='HasDetections'):\n","                if(col not in self.categorical_cols):\n","                    df[col].fillna(df[col].median(), inplace=True)\n","                else:\n","                    df[col]=self.l_encoder[col].fit_transform(df[col])\n","        \n","        df_X=df.drop(axis='columns', labels=['HasDetections'])\n","        df_Y=df['HasDetections']\n","        print(df_X.shape, df_Y.shape)\n","        \n","        df_X_cont=df_X.drop(axis='columns', labels=self.categorical_cols)\n","        df_X_cat=df_X[self.categorical_cols]\n","        print(df_X_cont.shape)\n","        print(df_X_cat.shape)\n","        \n","        ## checking whether attributes having type as \"object\"\n","        count=0\n","        for col in df_X.columns:\n","            if(df_X[col].dtype=='object'):\n","                count+=1\n","        print(\"Attribute counts that have type as Object: \", count)\n","        \n","        ## Outlier Removal [Need research]\n","        for col in df_X_cont.columns:\n","            if(col!='HasDetections'):\n","                outlierRem = self.OutlierRemoval(df_X_cont[col].quantile(0.25), df_X_cont[col].quantile(0.75))\n","                df_X_cont[col] = df_X_cont[col].apply(outlierRem.removeOutlier)\n","\n","        ## Standardisation of continuous valued attributes\n","        df_X_cont_scaled=self.robScaler.fit_transform(df_X_cont)\n","        df_X_cont_scaled=self.qTransform.fit_transform(df_X_cont_scaled)\n","        df_X_cont_scaled=pd.DataFrame(df_X_cont_scaled, index=df_X_cont.index, columns=df_X_cont.columns)\n","\n","        df_X=pd.concat([df_X_cont_scaled, df_X_cat], axis=1)\n","        \n","        # ## PCA decomposition of train data\n","        # self.pca.partial_fit(df_X_scaled)\n","\n","        df=pd.concat([df_X, df_Y], axis=1)\n","\n","        print(\"Data shape after preprocessing: \", df.shape)\n","\n","        ## data and label spliting from the dataset\n","        df_X = df.drop(axis='columns', labels=['HasDetections'])\n","        df_Y = df[['HasDetections']]\n","        print(df_X.shape, df_Y.shape)\n","\n","        x_train, x_test, y_train, y_test = model_selection.train_test_split(df_X, df_Y, train_size=0.8, stratify=df_Y)\n","\n","        return x_train, x_test, y_train, y_test\n","    \n","    ## preprocessing helper function for test data\n","    def test_preprocess(self, df_test):\n","        df_test.drop(axis='columns', labels=self.cols_to_drop, inplace=True)\n","        print(\"shape after dropping unnecesary columns: \", df_test.shape)\n","        \n","        for col in df_test.columns:\n","            if(len(df_test[col].unique())!=3 and df_test[col].dtype!='object'):\n","                df_test[col].fillna(df_test[col].median(), inplace=True)\n","            else:\n","                df_test[col]=self.l_encoder[col].fit_transform(df_test[col])\n","        \n","        df_test_cont=df_test.drop(axis='columns', labels=self.categorical_cols)\n","        df_test_cat=df_test[self.categorical_cols]\n","        print(df_test_cont.shape)\n","        print(df_test_cat.shape)\n","        \n","        for col in df_test_cont.columns:\n","            if(col!='HasDetections'):\n","                outlierRem = self.OutlierRemoval(df_test_cont[col].quantile(0.25), df_test_cont[col].quantile(0.75))\n","                df_test_cont[col] = df_test_cont[col].apply(outlierRem.removeOutlier)\n","                \n","        ## Standardisation of continuous valued attributes\n","        df_test_cont_scaled=self.robScaler.transform(df_test_cont)\n","        df_test_cont_scaled=self.qTransform.transform(df_test_cont)\n","        df_test_cont_scaled=pd.DataFrame(df_test_cont_scaled, index=df_test_cont.index, columns=df_test_cont.columns)\n","\n","        df_test_scaled=pd.concat([df_test_cont_scaled, df_test_cat], axis=1)\n","        \n","        return df_test_scaled.to_numpy()\n","        "]},{"cell_type":"markdown","source":["# Preprocessing Initialisation"],"metadata":{"id":"BqQyuaieMbWb"},"id":"BqQyuaieMbWb"},{"cell_type":"code","execution_count":null,"id":"5c641562","metadata":{"id":"5c641562"},"outputs":[],"source":["pre=preproc()"]},{"cell_type":"markdown","source":["**As Dataset has more than 7.1 million data rows, we follow chunk-wise data loading and preprocessing**\n","\n","So, here I loaded and preprocessed data in chunk size of 50,000"],"metadata":{"id":"i72tqpwNMhHt"},"id":"i72tqpwNMhHt"},{"cell_type":"code","execution_count":null,"id":"f0f0344a","metadata":{"scrolled":true,"id":"f0f0344a"},"outputs":[],"source":["# filename = 'model_lr.sav'\n","\n","X_train=[]\n","X_test=[]\n","Y_train=[]\n","Y_test=[]\n","\n","count=0\n","\n","for chunk_of_df in pd.read_csv(\"save-the-attack-contest/train_data.csv\", chunksize=50000):\n","    print(chunk_of_df.shape)\n","    \n","    x_train, x_test, y_train, y_test = pre.train_preprocess(chunk_of_df)\n","    \n","    X_train.append(x_train)\n","    X_test.append(x_test)\n","    Y_train.append(y_train)\n","    Y_test.append(y_test)\n","    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n","    \n","    count+=1"]},{"cell_type":"markdown","source":["# Training model declaration and training"],"metadata":{"id":"eDe0FOHNNB_-"},"id":"eDe0FOHNNB_-"},{"cell_type":"code","execution_count":null,"id":"59c2d141","metadata":{"id":"59c2d141"},"outputs":[],"source":["model_LR=linear_model.LogisticRegression(max_iter=500, verbose=2, class_weight='balanced')\n","\n","model_svm_linear=svm.SVC(kernel=\"linear\", probability=True, C=0.1)\n","\n","model_svm_rbf=svm.SVC(kernel=\"rbf\", probability=True, class_weight='balanced')\n","\n","model_BNB=BernoulliNB()\n","\n","clf_dt=DecisionTreeClassifier(random_state=0, class_weight='balanced')\n","\n","clf_rf=RandomForestClassifier(n_estimators=500, n_jobs=-1)\n","\n","clf_voting=VotingClassifier(\n","            estimators=[('lr', model_LR), ('rf', clf_rf)],\n","            voting='soft')\n","\n","clf_bagging=BaggingClassifier(\n","                DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n","                n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n","\n","ada_clf = AdaBoostClassifier(\n","            DecisionTreeClassifier(splitter=\"random\"), n_estimators=200,\n","            algorithm=\"SAMME.R\", learning_rate=0.5)"]},{"cell_type":"code","execution_count":null,"id":"6181b1df","metadata":{"id":"6181b1df"},"outputs":[],"source":["## RandomForestClassifier(n_estimators=500, n_jobs=-1)\n","\n","for i in range(count):\n","    clf_rf.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = clf_rf.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"4c6214a7","metadata":{"id":"4c6214a7"},"outputs":[],"source":["## LogisticRegression(max_iter=500, verbose=2, class_weight='balanced')\n","\n","for i in range(count):\n","    model_LR.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = model_LR.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"59489788","metadata":{"id":"59489788"},"outputs":[],"source":["## BernoulliNB()\n","\n","for i in range(count):\n","    model_BNB.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = model_BNB.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"c62fd2fe","metadata":{"id":"c62fd2fe"},"outputs":[],"source":["## DecisionTreeClassifier(random_state=0, class_weight='balanced')\n","\n","for i in range(count):\n","    clf_dt.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = clf_dt.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"e3b75937","metadata":{"id":"e3b75937"},"outputs":[],"source":["## VotingClassifier(\n","##            estimators=[('lr', model_LR), ('rf', clf_rf)],\n","##            voting='soft')\n","\n","for i in range(count):\n","    clf_voting.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = clf_voting.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"b19a5ca4","metadata":{"id":"b19a5ca4"},"outputs":[],"source":["## BaggingClassifier(\n","##                DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n","##                n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n","\n","for i in range(count):\n","    clf_bagging.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = clf_bagging.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"code","execution_count":null,"id":"3bbb9d3d","metadata":{"id":"3bbb9d3d"},"outputs":[],"source":["## AdaBoostClassifier(\n","##            DecisionTreeClassifier(splitter=\"random\"), n_estimators=200,\n","##            algorithm=\"SAMME.R\", learning_rate=0.5)\n","\n","for i in range(count):\n","    ada_clf.fit(X_train[i], Y_train[i])\n","    \n","    test_yhat  = ada_clf.predict_proba(X_test[i])[:,-1]\n","    print('intermediate accuracy : ', metric.roc_auc_score(Y_test[i], test_yhat))"]},{"cell_type":"markdown","source":["# Model Eveluation "],"metadata":{"id":"XN1TRPklNOxP"},"id":"XN1TRPklNOxP"},{"cell_type":"code","execution_count":null,"id":"7da49d42","metadata":{"id":"7da49d42"},"outputs":[],"source":["df_test=pd.read_csv(\"save-the-attack-contest/test_data.csv\")\n","df_test.head()"]},{"cell_type":"code","execution_count":null,"id":"d7c4beb7","metadata":{"id":"d7c4beb7"},"outputs":[],"source":["ids = df_test['MachineIdentifier']\n","ids.shape"]},{"cell_type":"code","execution_count":null,"id":"8ceedc52","metadata":{"id":"8ceedc52"},"outputs":[],"source":["X_test=pre.test_preprocess(df_test)\n","X_test.shape"]},{"cell_type":"code","execution_count":null,"id":"eb10fdea","metadata":{"id":"eb10fdea"},"outputs":[],"source":["y_pred_hat = clf_rf.predict_proba(df_test)[:, -1]\n","len(y_pred_hat)"]},{"cell_type":"code","execution_count":null,"id":"fac8ceef","metadata":{"id":"fac8ceef"},"outputs":[],"source":["submission_dict={\n","    \"MachineIdentifier\": ids,\n","    \"HasDetections\": y_pred_hat\n","}\n","sub_df=pd.DataFrame(submission_dict)\n","sub_df.head()"]},{"cell_type":"code","execution_count":null,"id":"70c0a15b","metadata":{"id":"70c0a15b"},"outputs":[],"source":["sub_df.to_csv('PO_sub_23_12_11_16_pm.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"2183b2cd","metadata":{"id":"2183b2cd"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"Save the Attack.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}